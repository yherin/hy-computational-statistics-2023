{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Helsinki, Master's Programme in Mathematics and Statistics  \n",
    "MAST32001 Computational Statistics, Autumn 2022  \n",
    "Luigi Acerbi  \n",
    "Based on notebook by Antti Honkela  \n",
    "\n",
    "# Lecture 5: Density estimation and cross validation\n",
    "\n",
    "Background reading: please see Chapter 5 of the \"Course notes\" available in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple 1D density estimation\n",
    "\n",
    "In this exercise, we perform simple density estimation for samples from a one-dimensional density.\n",
    "\n",
    "1. Generate 1000 samples from the standard (zero mean, unit variance) normal distribution $\\mathcal{N}(0,1)$.\n",
    "2. Plot the density and a normalized histogram of the samples (*Hint*: Use the option `density=True`).\n",
    "3. Estimate the density using the Gaussian kernel with bandwidth values $h=0.03, 0.1, 0.3, 1$.\n",
    "4. Plot the estimated densities in the same plot as the true density and the histogram. Which value seems the best?\n",
    "5. Try to further improve the estimate by changing $h$. How good can you make it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cross-validation to define the bandwidth $h$\n",
    "\n",
    "As we saw, the obvious drawback of the previous method is that the result is sensitive to the bandwidth $h$. A number of more or less heuristic methods for tuning the bandwidth have been developed. Here we will use a principled method based on leave-one-out cross-validation (LOO-CV). LOO-CV is based on the idea that we leave one point out when developing the model (here: defining the kernel density estimate) and test the model on the left-out point (here: evaluating the predictive probability of the left out point). This is repeated so that each point is left out in turn and the result is averaged over all.\n",
    "\n",
    "Formally: let's denote by $f_{\\setminus i,h}(x)$ the probability density fitted to $X \\setminus \\{ x_i \\}$. The LOO objective to maximize is\n",
    "$$ \\text{LOO}(h) = \\sum_{i=1}^n \\log f_{\\setminus i, h}(x_i).$$\n",
    "\n",
    "1. Apply cross-validation to find the optimal bandwidth in the previous exercise\n",
    "2. Plot the estimated density together with the true density and the normalized sample histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. More density estimation\n",
    "\n",
    "1. Generate a few data sets with different numbers of samples: 20, 150, 500 with the same normal distribution as above and apply the same procedure for kernel density estimation using the optimal LOO bandwidth. Does the optimal bandwidth change and if so, how?\n",
    "2. Generate data from different distributions (at least uniform and Laplace) and apply the same procedure. What do you observe?\n",
    "3. Generate data from the $d$-dimensional multivariate normal distribution with zero mean and unit covariance and $d = 3, 10, 20$. Estimate the density using the above procedure. Evaluate it at the points $(0, \\dots, 0)$ and $(1, \\dots, 1)$ and compare with the ground truth. What do you observe?\n",
    "\n",
    "*Hint*: In the $d$-variate case, use the multivariate normal $\\mathcal{N}(0, I_d)$ as the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Variants of cross-validation\n",
    "\n",
    "Leave-one-out cross-validation can be computationally demanding when the number of samples is large. In this task you will test a few alternatives on the case of problem 2.\n",
    "\n",
    "1. $k$-fold cross-validation: partition the data to $k$ disjoint blocks. Fit the density to $k-1$ blocks and compute the log-probability of the remaining block. Repeat $k$ times leaving each block for testing at the time. Implement $k$-fold cross-validation and test it with different values of $k$. (Note: this reduces to LOO-CV when $k=n$ for $n$ samples.)\n",
    "2. Monte Carlo cross-validation: randomly partition the data to a training set for fitting the density and a test set for evaluating the probability, with the desired split between training and test data. A split of 80% training / 20% testing is typical, but this can vary depending on data characteristics. Repeat until the desired level of accuracy has been reached. Implement Monte Carlo cross-validation and test it with different parameters.\n",
    "3. Which method seems most useful in getting accurate results quickly?\n",
    "\n",
    "*Hint*: One way to generate and access the $k$ blocks for $k$-fold cross-validation in Python is based on boolean index vectors. Assuming we have $N$ samples, the $i$th out of $k$ index vectors can be generated using\n",
    "``` {python}\n",
    "    testI = np.zeros(N, np.bool)\n",
    "    testI[((i*N)//k):(((i+1)*N)//k)] = True\n",
    "    trainI = ~testI\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "48562baf5ec4044852c40c81624b046fbb3741edb6df52b5d8ff6faefb753e30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
